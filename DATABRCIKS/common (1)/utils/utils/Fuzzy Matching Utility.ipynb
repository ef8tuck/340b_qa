{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85fc81f3-56a6-4fbd-ae9a-64123ce6bc9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ################################################## Module Information ###################################################\n",
    "#   Module Name         :   Fuzzy Matching of two datasets\n",
    "#   Description         :   This module is designed to perform fuzzy matching between two data tables based on specified\n",
    "#                           columns. The process includes:\n",
    "#                               a. Loading data from the input tables.\n",
    "#                               b. Takng distinct values from the targetted columns.\n",
    "#                               c. Data normalization (conversion to lowercase, trimming, removal of special characters).\n",
    "#                               d. Data reference optimization for parallel processing efficiency.\n",
    "#                               e. Fuzzy matching using Jaro-Winkler similarity for first comparison.\n",
    "#                               f. Selection of top-ranked matches based on given priority parameter.\n",
    "#                               g. Removal of block words and further data cleaning to improve accuracy.\n",
    "#                               h. Fuzzy mathcing using Jaccard Similarity to enhance matching results.\n",
    "#                               i. Filtering result based on similarity thresholds to refine results.\n",
    "#                               j. Compilation of final output including match scores and insights.\n",
    "#   \n",
    "#   Inputs              :   Two data tables for comparison, Fuzzy target columns, \n",
    "#                           Priority table for coverage(default: data-1), Threshold values(defaul present),  \n",
    "#                           Block words(optional)\n",
    "#   Outputs             :   A DataFrame containing matched product names\n",
    "#   Requirements        :   Python environment with necessary libraries.\n",
    "#   Last Modified       :   25 Sept, 2024\n",
    "#   Author              :   Rehan Mansoori\n",
    "#   Last modified By    :   Rehan Mansoori\n",
    "#   Change Log          :   Added Parallel Processing Enhancement\n",
    "# ######################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d886f68-0c7d-42de-86e0-98b6f0b1c483",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Install necessary Python packages for fuzzy matching and text processing\n",
    "!pip install python-Levenshtein\n",
    "!pip install fuzzywuzzy scikit-learn\n",
    "!pip install openpyxl\n",
    "\n",
    "# Import necessary libraries and functions\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import col, regexp_replace, udf, lower, split, explode, count, collect_list\n",
    "from collections import Counter\n",
    "import concurrent.futures\n",
    "from pyspark.sql.functions import col, row_number\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import explode, split, lower, col, count, regexp_replace\n",
    "from fuzzywuzzy import fuzz\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import jaccard_score\n",
    "import pandas as pd\n",
    "import openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85cac5a4-2eef-4a55-92b3-c1ebd89af720",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # Creating widgets for inputs\n",
    "    dbutils.widgets.text(\"User_table1_query\", \"\", \"1. Data 1 (Path/Query)\")\n",
    "    dbutils.widgets.text(\"User_table2_query\", \"\", \"2. Data 2 (Path/Query)\")\n",
    "    dbutils.widgets.text(\"Col_t1\",\"\", \"3. Match Column for Data 1\")\n",
    "    dbutils.widgets.text(\"Col_t2\",\"\", \"4. Match Column for Data 2\")\n",
    "\n",
    "    #Optional Value widget inputs\n",
    "    dbutils.widgets.text(\"Rank_col\", \"1\", \"Rank selection (Data: 1 or 2)\")\n",
    "    dbutils.widgets.text(\"block_words\", \"\", \"Comma-separated block words (optional)\")\n",
    "    dbutils.widgets.text(\"num_partitions\", \"28\", \"Number of partitions for Parallel processing\")\n",
    "    dbutils.widgets.text(\"jaccard_similarity_threshold\", \"0.25\", \"Jaccard similarity threshold\")\n",
    "    dbutils.widgets.text(\"jaro_winkler_similarity_threshold\", \"0.85\", \"Jaro-Winkler similarity threshold\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating widgets: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d67f3c37-91d1-4a82-a3b8-b69b0204f594",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Function to load data while handling case of CSV, txt, Parquet, Excel and SQL\n",
    "def load_data(query_or_path):\n",
    "    try:\n",
    "        if query_or_path.lower().endswith((\".csv\", \".txt\")):\n",
    "            return spark.read.options(header=True, quote='\"', multiLine=True, escape='\"').csv(query_or_path)\n",
    "        elif query_or_path.lower().endswith(\".parquet\"):\n",
    "            return spark.read.parquet(query_or_path)\n",
    "        elif query_or_path.lower().endswith((\".xlsx\", \".xls\")):\n",
    "            return spark.read.format(\"com.crealytics.spark.excel\").option(\"useHeader\", \"true\").option(\"treatEmptyValuesAsNulls\", \"true\").option(\"inferSchema\", \"true\").load(query_or_path)\n",
    "        else:\n",
    "            return spark.sql(query_or_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Retrieving widget values\n",
    "try:\n",
    "    user_table1_query = dbutils.widgets.get(\"User_table1_query\")\n",
    "    user_table2_query = dbutils.widgets.get(\"User_table2_query\")\n",
    "\n",
    "    user_table1 = load_data(user_table1_query)\n",
    "    user_table2 = load_data(user_table2_query)\n",
    "    col_t1 = dbutils.widgets.get(\"Col_t1\")\n",
    "    col_t2 = dbutils.widgets.get(\"Col_t2\")\n",
    "\n",
    "    Rank_col = dbutils.widgets.get(\"Rank_col\")\n",
    "    if Rank_col == \"1\":\n",
    "        rank_col = col_t1\n",
    "    else:\n",
    "        rank_col = col_t2\n",
    "\n",
    "    # Handling optional block words input\n",
    "    block_words_input = dbutils.widgets.get(\"block_words\")\n",
    "    if block_words_input:\n",
    "        block_words = block_words_input.split(\",\")\n",
    "    else:\n",
    "        block_words = []  # Default to empty list if not provided\n",
    "\n",
    "    # Retrieving number of partitions, Jaccard and Jaro-Winkler similarity thresholds\n",
    "    num_partitions = int(dbutils.widgets.get(\"num_partitions\"))\n",
    "    jaccard_similarity_threshold = float(dbutils.widgets.get(\"jaccard_similarity_threshold\"))\n",
    "    jaro_winkler_similarity_threshold = float(dbutils.widgets.get(\"jaro_winkler_similarity_threshold\"))\n",
    "except Exception as e:\n",
    "    print(f\"Error retrieving widget values or processing data: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52278e0b-ab93-4f13-b3e8-0b092064ae85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # Select distinct values from the specified column in tables\n",
    "    table1 = user_table1.select(col_t1).distinct()\n",
    "    table2 = user_table2.select(col_t2).distinct()\n",
    "\n",
    "    # Clean the columns in table1 by converting to lowercase and removing non-alphanumeric characters\n",
    "    table1 = table1.withColumn(f\"{col_t1}_prsd\", lower(trim(regexp_replace(col_t1, \"[^a-zA-Z0-9\\s]\", \"\"))))\n",
    "\n",
    "    # Clean the columns in table2 by converting to lowercase and removing non-alphanumeric characters\n",
    "    table2 = table2.withColumn(f\"{col_t2}_prsd\", lower(trim(regexp_replace(col_t2, \"[^a-zA-Z0-9\\s]\", \"\"))))\n",
    "\n",
    "    col_t1_prsd = f\"{col_t1}_prsd\"\n",
    "    col_t2_prsd = f\"{col_t2}_prsd\"\n",
    "\n",
    "    # Indicate preprocessing completion\n",
    "    print('Preprocessing done')\n",
    "\n",
    "    # Display counts of unique products in both DataFrames\n",
    "    table1_count = table1.count()\n",
    "    table2_count = table2.count()\n",
    "    print(\"table1 count : \"+ str(table1_count))\n",
    "    print(\"table2 count : \"+ str(table2_count))\n",
    "\n",
    "    # Automatically identify the table with greater count and exchange values if necessary\n",
    "    if table2_count > table1_count:\n",
    "        print(\"Exchanging data table references for Parallel Processing enhancement\")\n",
    "        table1, table2 = table2, table1\n",
    "        col_t1, col_t2 = col_t2, col_t1\n",
    "        col_t1_prsd, col_t2_prsd = col_t2_prsd, col_t1_prsd\n",
    "        if rank_col == col_t1:\n",
    "            rank_col = col_t2\n",
    "        else:\n",
    "            rank_col = col_t1\n",
    "except Exception as e:\n",
    "    print(f\"Error in preprocessing: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31a2af66-5ec8-48b8-b723-8d558a5a8d31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define utility functions for calculating maximum and minimum values\n",
    "def max_value(a, b):\n",
    "    \"\"\"Return the maximum of two values.\"\"\"\n",
    "    return a if a > b else b\n",
    "\n",
    "def min_value(a, b):\n",
    "    \"\"\"Return the minimum of two values.\"\"\"\n",
    "    return a if a < b else b\n",
    "\n",
    "# Define the Jaro-Winkler similarity function\n",
    "def jaro_winkler_similarity(s1, s2):\n",
    "    \"\"\"Calculate the Jaro-Winkler similarity between two strings.\"\"\"\n",
    "    try:\n",
    "        # Preprocess strings by stripping and converting to lowercase\n",
    "        s1 = s1.strip().lower()\n",
    "        s2 = s2.strip().lower()\n",
    "\n",
    "        # Initialize variables for matching characters\n",
    "        match_count = 0\n",
    "        s1_matches = [False] * len(s1)\n",
    "        s2_matches = [False] * len(s2)\n",
    "\n",
    "        # Calculate matching characters between s1 and s2\n",
    "        for i in range(len(s1)):\n",
    "            for j in range(len(s2)):\n",
    "                if not s2_matches[j] and s1[i] == s2[j]:\n",
    "                    s1_matches[i] = s2_matches[j] = True\n",
    "                    match_count += 1\n",
    "                    break\n",
    "\n",
    "        # Calculate transpositions\n",
    "        trans_count = 0\n",
    "        for i in range(len(s1)):\n",
    "            if i < len(s2) and s1_matches[i] and s2_matches[i] and s1[i] != s2[i]:\n",
    "                trans_count += 1\n",
    "\n",
    "        # Calculate Jaro similarity\n",
    "        if match_count == 0:\n",
    "            jaro_similarity = 0.0\n",
    "        else:\n",
    "            jaro_similarity = (match_count / len(s1) + match_count / len(s2) + (match_count - trans_count) / match_count) / 3.0\n",
    "\n",
    "        # Calculate prefix length for Jaro-Winkler adjustment\n",
    "        prefix_len = 0\n",
    "        for i in range(min_value(4, min_value(len(s1), len(s2)))):\n",
    "            if s1[i] == s2[i]:\n",
    "                prefix_len += 1\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        # Calculate Jaro-Winkler similarity\n",
    "        jaro_winkler_similarity = jaro_similarity + 0.1 * prefix_len * (1 - jaro_similarity)\n",
    "\n",
    "        return jaro_winkler_similarity\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating Jaro-Winkler similarity: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to process a partition of table1 and calculate similarity scores\n",
    "def process_partition(partition, table2_data):\n",
    "    \"\"\"Process a partition of table1 and calculate Jaro-Winkler similarity scores.\"\"\"\n",
    "    results = []\n",
    "    try:\n",
    "        for row1 in partition:\n",
    "            for row2 in table2_data:\n",
    "                similarity_score = jaro_winkler_similarity(row1[col_t1_prsd], row2[col_t2_prsd])\n",
    "                results.append((row1[col_t1], row2[col_t2], row1[col_t1_prsd], row2[col_t2_prsd], similarity_score))\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing partition: {e}\")\n",
    "    return results\n",
    "\n",
    "# Split table1 into partitions for parallel processing\n",
    "num_partitions = 28  # Number of partitions (threads) you want\n",
    "table1_partitions = table1.randomSplit([1.0 / num_partitions] * num_partitions)\n",
    "table2_data = table2.collect()  # Collect table2 cleaned data to use in all threads\n",
    "\n",
    "# Process each partition in parallel using ThreadPoolExecutor\n",
    "results = []\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=num_partitions) as executor:\n",
    "    future_to_partition = {executor.submit(process_partition, partition.collect(), table2_data): partition for partition in table1_partitions}\n",
    "    \n",
    "    for future in concurrent.futures.as_completed(future_to_partition):\n",
    "        try:\n",
    "            results.extend(future.result())\n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving results from future: {e}\")\n",
    "\n",
    "# Create a DataFrame from the results list\n",
    "result_df = spark.createDataFrame(results, [f'{col_t1}', f'{col_t2}', f'{col_t1_prsd}', f'{col_t2_prsd}', 'jaro_winkler_similarity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f68b763-a6ea-410a-9bac-fde08bd51c16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # Define a window specification to partition data by 'Product' and order by 'jaro_winkler_similarity' in descending order\n",
    "    window_spec = Window.partitionBy(rank_col).orderBy(col(\"jaro_winkler_similarity\").desc())\n",
    "\n",
    "    # Apply the window specification to rank the rows within each 'Product' group\n",
    "    ranked_df = result_df.withColumn(f\"ranked_{rank_col}\", row_number().over(window_spec))\n",
    "\n",
    "    # Filter to select only the top ranked record for each 'Product' group\n",
    "    rank1_rcds = ranked_df.filter(col(f\"ranked_{rank_col}\") == 1)\n",
    "\n",
    "    # Count the number of records in the 'Product' column of the rank1_rcds DataFrame\n",
    "    print(\"Count of df after Ranking: \"+ str(rank1_rcds.count()))\n",
    "    rank1_rcds.select(f'{rank_col}').count()\n",
    "except Exception as e:\n",
    "    print(f\"Error in ranking: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d91ada59-cff7-4d01-bea2-2f500ea6134f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # Step 3: Remove block words from the 'Product' column\n",
    "\n",
    "    # Create a regular expression pattern to match block words\n",
    "    block_words_expr = r'\\b(' + '|'.join(block_words) + r')\\b'\n",
    "\n",
    "    # Remove block words from the 'Product' column and create a new column 'Cleaned_Product'\n",
    "    rank1_rcds_cleaned = rank1_rcds.withColumn(f\"{col_t1_prsd}_cln\", regexp_replace(lower(col(col_t1_prsd)), block_words_expr, \"\"))\n",
    "    rank1_rcds_cleaned = rank1_rcds_cleaned.withColumn(f\"{col_t2_prsd}_cln\", regexp_replace(lower(col(col_t2_prsd)), block_words_expr, \"\"))\n",
    "\n",
    "    print(\"Count of df after Cleaning: \"+ str(rank1_rcds_cleaned.count()))\n",
    "\n",
    "    cln_col_t1 = f\"{col_t1_prsd}_cln\"\n",
    "    cln_col_t2 = f\"{col_t2_prsd}_cln\"\n",
    "except Exception as e:\n",
    "    print(f\"Error removing block words: {e}\")\n",
    "    rank1_rcds_cleaned = None\n",
    "    cln_col_t1 = None\n",
    "    cln_col_t2 = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1521ffd-4365-45a2-ab79-abf52df893e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Function to calculate Jaccard Similarity (Token-Based) with division by zero handling\n",
    "def jaccard_similarity(s1, s2):\n",
    "    try:\n",
    "        s1_tokens = set(s1.split())\n",
    "        s2_tokens = set(s2.split())\n",
    "        intersection = s1_tokens.intersection(s2_tokens)\n",
    "        union = s1_tokens.union(s2_tokens)\n",
    "        if len(union) == 0:\n",
    "            return 0  # or return 1, depending on the intended behavior for comparing two empty strings\n",
    "        return len(intersection) / len(union)\n",
    "    except ZeroDivisionError:\n",
    "        return 0\n",
    "\n",
    "# Sequential function to apply second algorithm on results from the first algorithm\n",
    "def apply_second_algorithm(rank1_rcds_cleaned):\n",
    "    rank1_rcds_cleaned = rank1_rcds_cleaned.toPandas()  # Convert Spark DataFrame to Pandas for easier manipulation\n",
    "\n",
    "    rank1_rcds_cleaned['jaccard_similarity'] = rank1_rcds_cleaned.apply(lambda row: jaccard_similarity(row[cln_col_t1], row[cln_col_t2]), axis=1)\n",
    "\n",
    "    return rank1_rcds_cleaned\n",
    "\n",
    "# Apply the second algorithm\n",
    "final_rank1_rcds_cleaned = apply_second_algorithm(rank1_rcds_cleaned)\n",
    "\n",
    "# Convert back to Spark DataFrame if needed\n",
    "final_result_spark_df = spark.createDataFrame(final_rank1_rcds_cleaned)\n",
    "\n",
    "final_result_spark_df = final_result_spark_df.select(f\"{col_t1}\", f\"{cln_col_t1}\", f\"{col_t2}\", f\"{cln_col_t2}\", \"jaro_winkler_similarity\", \"jaccard_similarity\")\n",
    "\n",
    "\n",
    "# Cast the 'jaccard_similarity' column to float type for consistency in numerical operations\n",
    "final_result_unfiltered_score = final_result_spark_df.withColumn(\"jaccard_similarity\", col(\"jaccard_similarity\").cast(\"float\")) \\\n",
    "                                                .withColumn(\"jaro_winkler_similarity\", col(\"jaro_winkler_similarity\").cast(\"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d844de3-a430-45be-9f68-2ab713cee75d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # Filter rows where jaccard_similarity is greater than 0\n",
    "    final_result_semifiltered_score = final_result_unfiltered_score.filter(col(\"jaccard_similarity\") > 0)\n",
    "\n",
    "    # Further filter the DataFrame to include rows where jaccard_similarity is at least 0.25 or jaro_winkler_similarity is at least 0.85\n",
    "    final_result_filtered_score = final_result_semifiltered_score.filter((col(\"jaccard_similarity\") >= jaccard_similarity_threshold) | (col(\"jaro_winkler_similarity\") >= jaro_winkler_similarity_threshold))\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f509452f-d9b8-4940-a55b-aebcffd25067",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # Select specific columns from the dataframe for further processing\n",
    "    # \"jaro_winkler_similarity\" and \"jaccard_similarity\" are metrics for comparing string similarity\n",
    "    final_result_filtered_score = final_result_filtered_score.select(f\"{col_t1}\", f\"{col_t2}\", f\"{cln_col_t1}\", f\"{cln_col_t2}\", f\"jaro_winkler_similarity\", f\"jaccard_similarity\")\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12e3937b-d93d-4a12-8833-d3f329de53f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    final_match = final_result_filtered_score.select(f\"{col_t1}\", f\"{col_t2}\")\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd2729d6-7cdb-429f-b5da-e1f43a37182e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_result_unfiltered_score.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1efe2ae-01d0-4e82-b797-50adc1a1370b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_result_filtered_score.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d99bb365-f35f-409b-bfd9-e1b066f556c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_match.display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Fuzzy Matching Utility",
   "widgets": {
    "Col_t1": {
     "currentValue": "Product",
     "nuid": "110d761b-5206-4d46-8bf8-4dd3852202ee",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "3. Match Column for Data 1",
      "name": "Col_t1",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "3. Match Column for Data 1",
      "name": "Col_t1",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "Col_t2": {
     "currentValue": "DRUG_NAME",
     "nuid": "fe284966-e629-4c1a-8a60-a30731ce879f",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "4. Match Column for Data 2",
      "name": "Col_t2",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "4. Match Column for Data 2",
      "name": "Col_t2",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "Rank_col": {
     "currentValue": "1",
     "nuid": "f2457ec5-9482-417d-b871-7354f950176e",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "1",
      "label": "Rank selection (Data: 1 or 2)",
      "name": "Rank_col",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "1",
      "label": "Rank selection (Data: 1 or 2)",
      "name": "Rank_col",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "User_table1_query": {
     "currentValue": "SELECT distinct Product FROM dev_ent_pl_thirdparty_db_1.evaluate_pharma.v_product_attribute WHERE PATENT_EXPIRY_DT is not null OR FIRST_LAUNCH_DT_USA is not null",
     "nuid": "354da941-b505-4bb4-b06f-0df012185665",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "1. Data 1 (Path/Query)",
      "name": "User_table1_query",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "1. Data 1 (Path/Query)",
      "name": "User_table1_query",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "User_table2_query": {
     "currentValue": "select distinct(DRUG_NAME), HCPCS_CD from dev_ent_pl_thirdparty_db_1.cms.v_asp_ndc_hcpcs_crosswalk_hist where DRUG_NAME is NOT NULL AND CURRENT_RECORD_IND=true",
     "nuid": "39ebc067-8e9e-4f35-a205-197b23b02ccc",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "2. Data 2 (Path/Query)",
      "name": "User_table2_query",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "2. Data 2 (Path/Query)",
      "name": "User_table2_query",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "block_words": {
     "currentValue": "research, program, project, hydrochloride, vaccine, sodium, cancer, sulfate, oral, iv, inhibitor, antibody, disease, injection, xr, cell, er, therapy, mab, acetate, acid, gel, cream, plus, tartrate, solution, in, citrate, dr, factor, dextrose, kit, vitamin, product, wound, depot, hcl, pak, for, g, matrix, insulin, testoterone, ablify, 300, 200",
     "nuid": "70f5fb3a-72f9-499b-8557-06abe3897c48",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Comma-separated block words (optional)",
      "name": "block_words",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Comma-separated block words (optional)",
      "name": "block_words",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "jaccard_similarity_threshold": {
     "currentValue": "0.25",
     "nuid": "61e5c048-cb32-4edd-9780-bf13935996d6",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "0.25",
      "label": "Jaccard similarity threshold",
      "name": "jaccard_similarity_threshold",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "0.25",
      "label": "Jaccard similarity threshold",
      "name": "jaccard_similarity_threshold",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "jaro_winkler_similarity_threshold": {
     "currentValue": "0.85",
     "nuid": "5ca29617-abde-4307-a3e4-e1228784043c",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "0.85",
      "label": "Jaro-Winkler similarity threshold",
      "name": "jaro_winkler_similarity_threshold",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "0.85",
      "label": "Jaro-Winkler similarity threshold",
      "name": "jaro_winkler_similarity_threshold",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "num_partitions": {
     "currentValue": "28",
     "nuid": "8ed3143f-b750-4bbb-b5f1-d98ed6720c7b",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "28",
      "label": "Number of partitions for Parallel processing",
      "name": "num_partitions",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "28",
      "label": "Number of partitions for Parallel processing",
      "name": "num_partitions",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
